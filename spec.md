# Weave Editor Technical Spec

Author: Isaac Jin

Version: 1.0

## System Introduction

### Brief View

![version-1](figures/version-1.svg)

The brief view of the system is shown above. The system can be divided into two phases, the **flow capturing phase** and the **policy generating phase**. A flow file will be generated by flow capturing and it will be read by policy generator to generate policies.

### Code Specification

The code hierarchy is shown below.

```
weave-editor/
├── master-node-codes
│   ├── scripts
│   │   ├── create_flow.sh
│   │   ├── send_data.sh
│   │   ├── start_proxy.sh
│   │   └── ws_server.py
│   └── weavescope-deploy
│       ├── latest.yaml
│       ├── original.yaml
│       └── version-1.yaml
├── processing-codes
│   ├── client-cap
│   │   ├── linker.py
│   │   └── ws_client.py
│   └── policy-generator
│       ├── generator.py
│       └── templates.py
└── figures, documents, etc.
```

#### Log Collector Script

The system requires a modified Weave Scope being deployed in the cluster. Weave Scope will have an agent deployed on each node and the pod can capture flows with eBPF. We modified the code of this pod so that it will print the flow it captured through log. We can use K8s commands to merge and collect logs from all the agent pods.

The log collector command can be found inside [send_data.sh](master-node-codes/scripts/send_data.sh).

#### Websocket Server

The websocket server corresponds to the file [ws_server.py](master-node-codes/scripts/ws_server.py). It takes the log collected by script as standard input and stores the log in a local list. Once the list has reached the specified size, the server will send the batch of flow logs to the websocket client and clear the list.

#### Websocket Client

The websocket client corresponds to the file [ws_client.py](processing-codes/client-cap/ws_client.py). Once it receives a batch of flow logs, it will request a topology JSON from the URI `api/report` of the HTTP server opened by `weave-scope-app`, which is a pod deployed on master node. The flow logs and the JSON containing topology information will then be sent to pod linker.

#### Pod Linker & Optimizer

The pod linker and optimizer are logically break into two modules but they are actually in one python file ([linker.py](processing-codes/client-cap/linker.py)). The linker will first parse the topology JSON to maintain a topology of all the resources in the cluster. Then it will link the endpoints of flow with pods and services. A flow will be output to the **Flow Record File** if and only if both of its endpoints can be linked to a pod.

#### Policy Generator

All the modules of policy generator is contained in one file [generator.py](processing-codes/policy-generator/generator.py). It will read the **Flow Record File** and try to aggregate the flows into rules (one rule will finally become one entry in the policy file). After that, it will aggregate rules into policies (one policy will finally become one policy YAML file). It will use two one-to-many mappings to maintain the relationships between them.

The policy generator contains a simple panel for user interaction. This panel is not coupled to the core code so you can remove it or modify it easily. You can also create your own UI with the API given. Some important APIs will be shown [there](#important-apis--classes).



## Modification to Weave Scope

We made some slight modification to weave scope to let it fit in our system. These modifications will not affect the normal use of Weave Scope **except for its logging system** (because we output a lot of flow there).

The modified Weave Scope can be found in [this repository](https://github.com/WHALEEYE/weave-scope). All the modified parts are marked with the following comment

````go
// ========= MODIFIED ==========
````

so you can easily find them by searching.

The meaning of the modified code will be commented nearby.



## Important APIs & Classes

### `linker.py`

#### Method `link_with_opt()`

Link the flow with pods and services, optimize and output processed flows.

##### Prototype

```python
def link_with_opt(records, report) -> list
```

##### Arguments

- `records`

  A list of strings, each string is one entry of flow log in one line. The format of one entry in flow log is

  ```
  [CONN] [eBPF] {hostname|timestamp|CPU|IPv4Event|pid|comm|Src|Dst|SrcPort|DstPort|NetNS|Fd}
  ```

  The script can also parse conntrack logs but we only care about eBPF records for now.

  A example of flow logs is provided [here](examples/flows.log).

- `report`

  A dictionary converted from `report.json`, which can be retrieved by requesting `api/report`.

  A example of `report.json` is provided [here](examples/report.json).

##### Returns

A list of flow records. The websocket client will then write the records into **Flow Record File**.

The flow record file should contain one flow record in one line. Each flow record is a JSON object with information about the source and destination of the flow, associated with pod and service information.

An example of flow record file is provided [here](examples/flows.json).



### `generator.py`

#### Method `aggregate_conn()`

Aggregate flows into connections.

##### Prototype

```python
def aggregate_conn(flow) -> None
```

##### Arguments

- `flow`

  A list of flow records. The format is the same with the list returned by `link_with_opt()`.

##### Returns

This function do not have return values but it will push the processed data into a global data structure.



#### Class `ServiceEnd`

Represents a service endpoint, containing service ID, name, namespace, IP, port and selector.

The IP is used to describe the endpoints outside the cluster (CIDR), which is not used in the current version.

##### Attributes

- `svc_id`

  Service ID get from `report.json`.

- `svc_name`

- `ns`

  The namespace of the service.

- `ip`

  If the endpoint comes from outside the cluster, use this attribute to describe.

- `port `

- `selectors `

  A dictionary, represents the pod selector of this service.



#### Class `Connection`

Represents a connection, containing source and destination service endpoints, and direction.

**This is the class that directly interacts with user.**

##### Attributes

- `src` and `dst`

  Source service and destination service. Both of them are objects of `ServiceEnd`.

- `direction`

  Ingress or egress. There is a `Direction` enumeration type defined for this.

##### Class Methods

- `activate()`

  **Prototype**

  ```python
  def activate(self) -> None
  ```

  **Description**

  Activates this connection. If the user selects one connection, this is the method that should be called.

- `deactivate()`

  **Prototype**

  ```python
  def deactivate(self) -> None
  ```

  **Description**

  Deactivates this connection. If the user cancels one connection, this is the method that should be called.



#### Method `get_connections()`

Get the aggregated connections.

##### Prototype

```python
def get_connections() -> list
```

##### Returns

A list of `Connection` objects. This method should be called when displaying connection to user for choosing, and the information of each connection can be get from the `Connection` objects. You can refer to method `display()` as an example of displaying information of connections.



#### Method `generate_policy_yaml()`

Check all the policies and generate YAML strings for activated policy.

##### Prototype

```python
def generate_policy_yaml() -> dict
```

##### Returns

A dictionary whose key is the name of the policy and value is the YAML file content.



## Future Works

### Integrate Server into WeaveScope

Now the system is only a external system who collects flow through WebSocket communication with WeaveScope, which is pretty unstable and incoherent. Also, since the flows are output to log, the normal use of logs in Weave Scope is severely affected.

Better to implement the flow collecting and sending module inside weave scope and expose an API (better websocket) for the client to connect, so that the whole system will be more stable and can be used in producing environment.

### Add Graphic UI

The system needs a GUI because

- There is an inherent drawback of flow analyzing that flow capturing can not cover all the possible flows in the cluster. When it comes to generating policy, the drawback is fatal if we ignore it because the policy of K8s is in white-list manner, which means that the possible flows not covered will be blocked by default and it will cause severe consequences.
- Our system can only capture and extract the flow between services inside cluster, and it can't aggregate the flow that comes from the internet, so that the user must manually add ingress and egress policy which is outside the cluster.

From above we can see that we must need a GUI to allow the user to adjust the generated policy.
